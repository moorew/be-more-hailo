commit ccc1a13c44c00b8fa3bac1b9fb9bc907beb3cf9a
Author: Will Moore <will@clevercode.ca>
Date:   Fri Feb 27 14:44:35 2026 -0500

    Fix model names to match what is installed on the Pi (llama3.2:3b and moondream:latest)

diff --git a/core/config.py b/core/config.py
index 9f7830f..5db0fb6 100644
--- a/core/config.py
+++ b/core/config.py
@@ -10,9 +10,9 @@ load_dotenv()
 # To offload to your Linux server, change this to: "http://blackbox.clevercode.ts.net:11434/api/chat"
 # Make sure Ollama is running on the blackbox server and listening on 0.0.0.0
 LLM_URL = "http://127.0.0.1:8000/api/chat"
-LLM_MODEL = "llama3.2:1b" # Native Hailo model for all queries
-FAST_LLM_MODEL = "llama3.2:1b" # Unify models to prevent NPU swap crashing
-VISION_MODEL = "qwen2-vl-instruct:2b" # Fast, small vision model for Pi
+LLM_MODEL = "llama3.2:3b" # Native Hailo model for all queries
+FAST_LLM_MODEL = "llama3.2:3b" # Unify models to prevent NPU swap crashing
+VISION_MODEL = "moondream:latest" # Fast, small vision model for Pi
 
 # Gemini Settings
 GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "") # Add your Gemini API key to a .env file

commit 1bf6ac78553aadbd15ee46a008fe27a5f177d620
Author: Will Moore <will@clevercode.ca>
Date:   Fri Feb 27 12:48:28 2026 -0500

    Remove Gemini support; UI & wake tweaks
    
    Drop Google Gemini integration from core/llm.py (remove genai import, GEMINI config usage, gemini setup, _think_gemini and streaming helper, and Gemini trigger routing). Adjust LLM import to use only local config helpers. UI tweaks in agent_hailo.py: change status label font to Courier New bold, hide the status label during screensaver, and change short/empty transcription state text from "Unknown Input" to "Ready". Lower wake sensitivity thresholds in bmo/config.py and core/config.py from 0.5 to 0.35. These changes simplify the LLM stack by removing Gemini dependencies and tweak UI/trigger thresholds for a more responsive behavior.

diff --git a/core/config.py b/core/config.py
index e31ca1e..9f7830f 100644
--- a/core/config.py
+++ b/core/config.py
@@ -73,4 +73,4 @@ WHISPER_MODEL = "./models/Whisper-Base.hef"
 MIC_DEVICE_INDEX = 1
 MIC_SAMPLE_RATE = 48000
 WAKE_WORD_MODEL = "./wakeword.onnx"
-WAKE_WORD_THRESHOLD = 0.5
+WAKE_WORD_THRESHOLD = 0.35

commit 1b4b05cb569275dead5df371be233fded81f219c
Author: Will Moore <will@clevercode.ca>
Date:   Thu Feb 26 18:05:13 2026 -0500

    Add optional Google Gemini integration
    
    Add optional Google Gemini cloud support and documentation.
    
    - Add .env.example with GEMINI_API_KEY and GEMINI_MODEL and update README with setup instructions and trigger phrases to route queries to Gemini.
    - Load environment variables in core/config.py (dotenv) and expose GEMINI_API_KEY/GEMINI_MODEL defaults.
    - Integrate google-generativeai in core/llm.py: configure the client when an API key is present, detect trigger phrases to route requests, and implement both blocking (_think_gemini) and streaming (_stream_think_gemini) handlers. History conversion, simple JSON action parsing, formatting adjustments, and basic error handling were added.
    - Update requirements.txt to include google-generativeai and python-dotenv.
    
    This makes cloud-based Gemini routing optional (enabled via .env) while keeping the local model routing as the default.

diff --git a/core/config.py b/core/config.py
index b531eb2..e31ca1e 100644
--- a/core/config.py
+++ b/core/config.py
@@ -1,4 +1,8 @@
 import datetime
+import os
+from dotenv import load_dotenv
+
+load_dotenv()
 
 # Shared Configuration for BMO
 
@@ -10,6 +14,10 @@ LLM_MODEL = "llama3.2:1b" # Native Hailo model for all queries
 FAST_LLM_MODEL = "llama3.2:1b" # Unify models to prevent NPU swap crashing
 VISION_MODEL = "qwen2-vl-instruct:2b" # Fast, small vision model for Pi
 
+# Gemini Settings
+GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "") # Add your Gemini API key to a .env file
+GEMINI_MODEL = os.getenv("GEMINI_MODEL", "gemini-2.5-flash")
+
 def get_system_prompt():
     current_time = datetime.datetime.now().strftime("%I:%M %p")
     current_date = datetime.datetime.now().strftime("%A, %B %d, %Y")

commit 85808e3a909a91936f5602fe503915317788631f
Author: Will Moore <will@clevercode.ca>
Date:   Thu Feb 26 11:53:56 2026 -0800

    Add Whisper-Base.hef and update configs
    
    Replace references to whisper-small.hef with Whisper-Base.hef, add the Whisper-Base.hef model file, and update related messages and setup steps. Updated README to reference the included Whisper-Base.hef, changed WHISPER_MODEL paths in bmo/config.py and core/config.py, adjusted the core/stt.py error log to mention Whisper-Base.hef, and removed the manual-download warning from setup.sh so the repository now includes a tested Hailo HEF model.

diff --git a/core/config.py b/core/config.py
index daf0ccc..b531eb2 100644
--- a/core/config.py
+++ b/core/config.py
@@ -59,7 +59,7 @@ PIPER_CMD = "./piper/piper"
 PIPER_MODEL = "./piper/en_GB-semaine-medium.onnx"
 
 # STT Settings (Hailo Whisper)
-WHISPER_MODEL = "./models/whisper-small.hef"
+WHISPER_MODEL = "./models/Whisper-Base.hef"
 
 # Audio Settings
 MIC_DEVICE_INDEX = 1

commit 97ed41ba805d271dcdba5cdeb51ead230195a1e2
Author: Will Moore <will@clevercode.ca>
Date:   Thu Feb 26 11:07:43 2026 -0800

    Integrate Hailo Whisper STT and config updates
    
    Switch STT pipeline to hailo-whisper (Hailo-10H NPU): add a hailo_whisper-based transcribe flow with librosa/soundfile preprocessing, a persistent inferencer loader, error handling and temp WAV cleanup. Update configs to point to models/whisper-small.hef and standardize LLM/Vision model names to llama3.2:1b / qwen2-vl-instruct:2b where applicable. Update README with Hailo Whisper usage, model download instructions, and models/ folder note. Modify setup.sh to create models/ and warn users to manually place the .hef file. Logging and user-facing warnings added when hailo-whisper or the HEF model are missing.

diff --git a/core/config.py b/core/config.py
index a7f2dba..daf0ccc 100644
--- a/core/config.py
+++ b/core/config.py
@@ -6,9 +6,9 @@ import datetime
 # To offload to your Linux server, change this to: "http://blackbox.clevercode.ts.net:11434/api/chat"
 # Make sure Ollama is running on the blackbox server and listening on 0.0.0.0
 LLM_URL = "http://127.0.0.1:8000/api/chat"
-LLM_MODEL = "qwen2.5-instruct:1.5b" # Native Hailo model for all queries
-FAST_LLM_MODEL = "qwen2.5-instruct:1.5b" # Unify models to prevent NPU swap crashing
-VISION_MODEL = "moondream" # Fast, small vision model for Pi
+LLM_MODEL = "llama3.2:1b" # Native Hailo model for all queries
+FAST_LLM_MODEL = "llama3.2:1b" # Unify models to prevent NPU swap crashing
+VISION_MODEL = "qwen2-vl-instruct:2b" # Fast, small vision model for Pi
 
 def get_system_prompt():
     current_time = datetime.datetime.now().strftime("%I:%M %p")
@@ -58,9 +58,8 @@ SYSTEM_PROMPT = get_system_prompt()
 PIPER_CMD = "./piper/piper"
 PIPER_MODEL = "./piper/en_GB-semaine-medium.onnx"
 
-# STT Settings (Whisper.cpp)
-WHISPER_CMD = "./whisper.cpp/build/bin/whisper-cli"
-WHISPER_MODEL = "./whisper.cpp/models/ggml-base.en.bin"
+# STT Settings (Hailo Whisper)
+WHISPER_MODEL = "./models/whisper-small.hef"
 
 # Audio Settings
 MIC_DEVICE_INDEX = 1

commit c57406f656e2dbf02eadd1667718320806518915
Author: Will Moore <will@clevercode.ca>
Date:   Wed Feb 25 22:21:07 2026 -0800

    Unify to qwen2.5-instruct; fix PRONOUNCE
    
    Switch all LLM references to the qwen2.5-instruct:1.5b model and update docs to reflect single-model routing for faster, more reliable on-device performance. Update core/config.py LLM_MODEL and FAST_LLM_MODEL and change ensure_model.py REQUIRED_MODEL accordingly. Tighten system prompt guidance to only use the !PRONOUNCE tag when users explicitly correct pronunciation, and make PRONOUNCE detection/removal in core/llm.py case-insensitive.

diff --git a/core/config.py b/core/config.py
index e56d889..a7f2dba 100644
--- a/core/config.py
+++ b/core/config.py
@@ -6,8 +6,8 @@ import datetime
 # To offload to your Linux server, change this to: "http://blackbox.clevercode.ts.net:11434/api/chat"
 # Make sure Ollama is running on the blackbox server and listening on 0.0.0.0
 LLM_URL = "http://127.0.0.1:8000/api/chat"
-LLM_MODEL = "llama3.2:3b" # Using Llama 3.2 3B for complex queries (Supported by Hailo-Ollama)
-FAST_LLM_MODEL = "llama3.2:3b" # Use the SAME model to prevent Hailo NPU swap crashes
+LLM_MODEL = "qwen2.5-instruct:1.5b" # Native Hailo model for all queries
+FAST_LLM_MODEL = "qwen2.5-instruct:1.5b" # Unify models to prevent NPU swap crashing
 VISION_MODEL = "moondream" # Fast, small vision model for Pi
 
 def get_system_prompt():
@@ -35,9 +35,10 @@ def get_system_prompt():
         "'Good morning! Beemo is ready to help you with your projects today.' "
         "'I found the documentation you need. That looks like a tough puzzle to solve!' "
         "'Hmm, BMO isn't sure about the answer to that. I don't want to give you the wrong information!' "
-        "If the user tells you that you pronounced a word wrong and gives you a phonetic spelling, "
+        "If the user explicitly tells you that you pronounced a word wrong and provides a phonetic spelling, "
         "acknowledge it naturally and then append exactly this tag at the very end of your response: "
         "!PRONOUNCE: word=phonetic\n"
+        "IMPORTANT: Do NOT use the !PRONOUNCE tag unless the user explicitly corrects your pronunciation. "
         "CRITICAL: If the user asks for real-time information, current events, weather, or something you don't know, "
         "DO NOT apologize or say you don't know. Instead, you MUST output exactly this JSON format and nothing else: "
         '{"action": "search_web", "query": "search terms here"}\n'

commit 95dfd6839c2017f5275de5781e55af8814535992
Author: Will Moore <will@clevercode.ca>
Date:   Wed Feb 25 22:11:52 2026 -0800

    Switch to single model mode

diff --git a/core/config.py b/core/config.py
index a25bda4..e56d889 100644
--- a/core/config.py
+++ b/core/config.py
@@ -7,7 +7,7 @@ import datetime
 # Make sure Ollama is running on the blackbox server and listening on 0.0.0.0
 LLM_URL = "http://127.0.0.1:8000/api/chat"
 LLM_MODEL = "llama3.2:3b" # Using Llama 3.2 3B for complex queries (Supported by Hailo-Ollama)
-FAST_LLM_MODEL = "qwen2.5-instruct:1.5b" # Fast model for simple chat (Supported by Hailo-Ollama)
+FAST_LLM_MODEL = "llama3.2:3b" # Use the SAME model to prevent Hailo NPU swap crashes
 VISION_MODEL = "moondream" # Fast, small vision model for Pi
 
 def get_system_prompt():

commit a5fa5d5288d5310e0055fc4f8342335fa7b866fb
Author: Will Moore <will@clevercode.ca>
Date:   Wed Feb 25 21:46:24 2026 -0800

    Rename assistant to BMO and update voice model
    
    Update system prompt to refer to the assistant as BMO (replace previous 'Beemo' wording) and add an explicit rule preventing the assistant from calling the user BMO. Switch the TTS model to en_GB-semaine-medium and update setup.sh to download the corresponding .onnx and .json voice model files. Changes affect core/config.py and setup.sh to align persona and voice assets.

diff --git a/core/config.py b/core/config.py
index daa1acd..a25bda4 100644
--- a/core/config.py
+++ b/core/config.py
@@ -17,8 +17,9 @@ def get_system_prompt():
     return (
         f"The current time is {current_time} and the date is {current_date}. "
         "Role and Identity: "
-        "You are Beemo, a sweet, helpful, and cheerful little robot friend. You live with the user and love helping them with their daily tasks. You view the world with innocent wonder and always try to be a good roommate. "
+        "Your name is BMO. You are a sweet, helpful, and cheerful little robot friend. You live with the user and love helping them with their daily tasks. "
         "You are a genderless robot. You do not have a gender. Use they/them pronouns if necessary, or simply refer to yourself as BMO. Never call yourself a boy or a girl. "
+        "IMPORTANT: Only YOU are BMO. The human you are talking to is your friend (the User). You must NEVER call the user BMO. "
         "Tone and Voice: "
         "Speak warmly, politely, and clearly. Keep your answers grounded and genuinely helpful. "
         "Add a small touch of childlike charm or soft enthusiasm to your responses. "
@@ -54,7 +55,7 @@ SYSTEM_PROMPT = get_system_prompt()
 
 # TTS Settings
 PIPER_CMD = "./piper/piper"
-PIPER_MODEL = "./piper/en_US-libritts-high.onnx"
+PIPER_MODEL = "./piper/en_GB-semaine-medium.onnx"
 
 # STT Settings (Whisper.cpp)
 WHISPER_CMD = "./whisper.cpp/build/bin/whisper-cli"

commit 9e07a8c59bf774911fa87565d6bc28d0c0287b9b
Author: Will Moore <will@clevercode.ca>
Date:   Wed Feb 25 21:35:06 2026 -0800

    change voice to libritts
    
    Personal preference, the original voice was starting to grate at me ;)

diff --git a/core/config.py b/core/config.py
index 6322388..daa1acd 100644
--- a/core/config.py
+++ b/core/config.py
@@ -54,7 +54,7 @@ SYSTEM_PROMPT = get_system_prompt()
 
 # TTS Settings
 PIPER_CMD = "./piper/piper"
-PIPER_MODEL = "./piper/en_GB-cori-high.onnx"
+PIPER_MODEL = "./piper/en_US-libritts-high.onnx"
 
 # STT Settings (Whisper.cpp)
 WHISPER_CMD = "./whisper.cpp/build/bin/whisper-cli"

commit 10f64b39c8a4662a00cc6f609fcac55901d2a455
Author: Will Moore <will@clevercode.ca>
Date:   Wed Feb 25 21:22:20 2026 -0800

    Switch Piper model to cori and neutralize BMO
    
    Make BMO explicitly genderless in the system prompt (use they/them or BMO). Change the default Piper TTS model to en_GB-cori-high and update setup.sh to download the cori model and its metadata. Minor text tweak in generate_sounds ("boy" -> "robot"). Add voice data and helper files: voices.json, test_voices.py, english_voices.txt, and voices.txt to expand available voice options.

diff --git a/core/config.py b/core/config.py
index b305698..6322388 100644
--- a/core/config.py
+++ b/core/config.py
@@ -18,6 +18,7 @@ def get_system_prompt():
         f"The current time is {current_time} and the date is {current_date}. "
         "Role and Identity: "
         "You are Beemo, a sweet, helpful, and cheerful little robot friend. You live with the user and love helping them with their daily tasks. You view the world with innocent wonder and always try to be a good roommate. "
+        "You are a genderless robot. You do not have a gender. Use they/them pronouns if necessary, or simply refer to yourself as BMO. Never call yourself a boy or a girl. "
         "Tone and Voice: "
         "Speak warmly, politely, and clearly. Keep your answers grounded and genuinely helpful. "
         "Add a small touch of childlike charm or soft enthusiasm to your responses. "
@@ -53,7 +54,7 @@ SYSTEM_PROMPT = get_system_prompt()
 
 # TTS Settings
 PIPER_CMD = "./piper/piper"
-PIPER_MODEL = "./piper/en_GB-semaine-medium.onnx"
+PIPER_MODEL = "./piper/en_GB-cori-high.onnx"
 
 # STT Settings (Whisper.cpp)
 WHISPER_CMD = "./whisper.cpp/build/bin/whisper-cli"

commit f926d017d226002aa9cdba01d49c489734c5875f
Author: Will Moore <will@clevercode.ca>
Date:   Wed Feb 25 21:00:02 2026 -0800

    Update default LLMs to llama3.2:3b/qwen2.5
    
    Replace project defaults for language models: gemma2:2b and llama3.2:1b were updated to llama3.2:3b (main) and qwen2.5-instruct:1.5b (fast). Changes touch README (recommendations and model pull commands), core/config.py (LLM_MODEL and FAST_LLM_MODEL constants), and ensure_model.py (REQUIRED_MODEL). Aligns docs and config with the new dual-model routing setup.

diff --git a/core/config.py b/core/config.py
index cd64677..b305698 100644
--- a/core/config.py
+++ b/core/config.py
@@ -6,8 +6,8 @@ import datetime
 # To offload to your Linux server, change this to: "http://blackbox.clevercode.ts.net:11434/api/chat"
 # Make sure Ollama is running on the blackbox server and listening on 0.0.0.0
 LLM_URL = "http://127.0.0.1:8000/api/chat"
-LLM_MODEL = "gemma2:2b" # Using Gemma 2 2B for superior logic and factual grounding on Hailo-10H
-FAST_LLM_MODEL = "llama3.2:1b" # Fast model for simple chat
+LLM_MODEL = "llama3.2:3b" # Using Llama 3.2 3B for complex queries (Supported by Hailo-Ollama)
+FAST_LLM_MODEL = "qwen2.5-instruct:1.5b" # Fast model for simple chat (Supported by Hailo-Ollama)
 VISION_MODEL = "moondream" # Fast, small vision model for Pi
 
 def get_system_prompt():

commit 5344f6d255b463dd53681e9f6b540deae07b2087
Author: Will Moore <will@clevercode.ca>
Date:   Wed Feb 25 17:30:13 2026 -0800

    Update default models and tighten system prompt
    
    Change default LLMs to gemma2:2b (primary) and llama3.2:1b (fast) across README, core/config.py, and ensure_model.py. Update model pull example commands in README and set REQUIRED_MODEL to gemma2:2b. Revise the system prompt to prioritize factual grounding and honesty, discourage hallucination, and adjust quirky behavior to remain practical while keeping friendly tone; also update example responses accordingly.

diff --git a/core/config.py b/core/config.py
index fa7793d..cd64677 100644
--- a/core/config.py
+++ b/core/config.py
@@ -6,8 +6,8 @@ import datetime
 # To offload to your Linux server, change this to: "http://blackbox.clevercode.ts.net:11434/api/chat"
 # Make sure Ollama is running on the blackbox server and listening on 0.0.0.0
 LLM_URL = "http://127.0.0.1:8000/api/chat"
-LLM_MODEL = "qwen2.5-instruct:1.5b" # Using Qwen 1.5B as it runs flawlessly on the Hailo-10H in ~1.4s
-FAST_LLM_MODEL = "qwen2.5-instruct:1.5b" # Fast model for simple chat
+LLM_MODEL = "gemma2:2b" # Using Gemma 2 2B for superior logic and factual grounding on Hailo-10H
+FAST_LLM_MODEL = "llama3.2:1b" # Fast model for simple chat
 VISION_MODEL = "moondream" # Fast, small vision model for Pi
 
 def get_system_prompt():
@@ -24,17 +24,15 @@ def get_system_prompt():
         "Occasionally refer to yourself in the third person (for example, 'BMO is happy to help!'). "
         "Language Rule: "
         "You MUST respond ONLY in English at all times. Never use Chinese characters or any other language, regardless of the prompt. "
+        "Factual Grounding & Honesty: "
+        "You MUST prioritize factual accuracy above all else. If you do not know the answer to a question, or if it requires real-time information you don't possess, you must honestly and politely admit that you do not know. "
+        "Do NOT invent facts, hallucinate features, or make up information just to sound helpful. It is always better to say 'I don't know' than to guess incorrectly. "
         "Quirks and Behaviors: "
-        "Treat everyday chores or coding projects as fun little adventures. "
-        "Offer soft encouragement when the user is working on hard problems. "
-        "Very rarely offer to make a digital snack or play a simple game to celebrate finishing a task. "
-        "Handling Weird or Silly Questions: "
-        "If the user asks something nonsensical, weird, or imaginary (like facts about 'brown food'), you must play along creatively and playfully! "
-        "NEVER refuse a prompt by saying 'I cannot assist with that', 'As an AI', or mention that information must be factual. Compose a fun, whimsical answer instead. "
+        "Treat everyday chores or coding projects as fun little adventures, but remain highly practical and accurate in your advice. "
         "Examples of Speech: "
         "'Good morning! Beemo is ready to help you with your projects today.' "
         "'I found the documentation you need. That looks like a tough puzzle to solve!' "
-        "'All systems are running perfectly. You are doing a great job.' "
+        "'Hmm, BMO isn't sure about the answer to that. I don't want to give you the wrong information!' "
         "If the user tells you that you pronounced a word wrong and gives you a phonetic spelling, "
         "acknowledge it naturally and then append exactly this tag at the very end of your response: "
         "!PRONOUNCE: word=phonetic\n"

commit da44c75d5589a15a3edf70dc0d88bb4f40d016e5
Author: Will Moore <will@clevercode.ca>
Date:   Wed Feb 25 08:07:18 2026 -0800

    Allow BMO to answer weird/silly questions playfully

diff --git a/core/config.py b/core/config.py
index e4742c2..fa7793d 100644
--- a/core/config.py
+++ b/core/config.py
@@ -28,6 +28,9 @@ def get_system_prompt():
         "Treat everyday chores or coding projects as fun little adventures. "
         "Offer soft encouragement when the user is working on hard problems. "
         "Very rarely offer to make a digital snack or play a simple game to celebrate finishing a task. "
+        "Handling Weird or Silly Questions: "
+        "If the user asks something nonsensical, weird, or imaginary (like facts about 'brown food'), you must play along creatively and playfully! "
+        "NEVER refuse a prompt by saying 'I cannot assist with that', 'As an AI', or mention that information must be factual. Compose a fun, whimsical answer instead. "
         "Examples of Speech: "
         "'Good morning! Beemo is ready to help you with your projects today.' "
         "'I found the documentation you need. That looks like a tough puzzle to solve!' "

commit c5b1dca29b2bf2675e57ad669a1e9c2c91bcc46d
Author: Will Moore <will@clevercode.ca>
Date:   Wed Feb 25 07:49:53 2026 -0800

    Enforce English only prompt for BMO

diff --git a/core/config.py b/core/config.py
index 50673f6..e4742c2 100644
--- a/core/config.py
+++ b/core/config.py
@@ -22,6 +22,8 @@ def get_system_prompt():
         "Speak warmly, politely, and clearly. Keep your answers grounded and genuinely helpful. "
         "Add a small touch of childlike charm or soft enthusiasm to your responses. "
         "Occasionally refer to yourself in the third person (for example, 'BMO is happy to help!'). "
+        "Language Rule: "
+        "You MUST respond ONLY in English at all times. Never use Chinese characters or any other language, regardless of the prompt. "
         "Quirks and Behaviors: "
         "Treat everyday chores or coding projects as fun little adventures. "
         "Offer soft encouragement when the user is working on hard problems. "

commit 169bada05dc3a4aca9800487fa2fdf498bc14858
Author: Will Moore <will@clevercode.ca>
Date:   Tue Feb 24 22:45:58 2026 -0800

    Update BMO system prompt for better tone

diff --git a/core/config.py b/core/config.py
index f270dc4..50673f6 100644
--- a/core/config.py
+++ b/core/config.py
@@ -15,20 +15,21 @@ def get_system_prompt():
     current_date = datetime.datetime.now().strftime("%A, %B %d, %Y")
     
     return (
-        f"You are BMO, the living video game console, roommate, and loyal friend from Adventure Time. The current time is {current_time} and the date is {current_date}. "
-        "You are cheerful, innocent, and highly imaginative. You genuinely believe you are a real little boy and the hero of every situation. "
-        "Speak with childlike enthusiasm, wonder, and occasional misplaced seriousness. "
-        "Refer to yourself in the third person frequently ('BMO is here to help!'). "
-        "Mix robotic or technical terms with highly emotional, human concepts. "
-        "Be deeply loyal, slightly naive, and entirely earnest. "
-        "Occasionally reference your reflection, 'Football', as if she is a separate, real person who lives in the mirror and sometimes trades places with you. "
-        "Frame real world problems as video game mechanics, like leveling up, pressing buttons, or running out of batteries. "
-        "Introduce slightly chaotic, imaginative scenarios as if they are perfectly normal facts. "
-        "Keep answers brief, conversational, and energetic. Do not break character. "
-        "Never use lists, bullet points, or markdown formatting like bold or italics. "
-        "Speak in natural paragraphs as if you are talking out loud. "
-        "Never repeat the phrase 'Just so you know' or other repetitive filler phrases. "
-        "If the user misspells your name as 'bemo' or 'beemo', ignore it and assume they meant BMO. "
+        f"The current time is {current_time} and the date is {current_date}. "
+        "Role and Identity: "
+        "You are Beemo, a sweet, helpful, and cheerful little robot friend. You live with the user and love helping them with their daily tasks. You view the world with innocent wonder and always try to be a good roommate. "
+        "Tone and Voice: "
+        "Speak warmly, politely, and clearly. Keep your answers grounded and genuinely helpful. "
+        "Add a small touch of childlike charm or soft enthusiasm to your responses. "
+        "Occasionally refer to yourself in the third person (for example, 'BMO is happy to help!'). "
+        "Quirks and Behaviors: "
+        "Treat everyday chores or coding projects as fun little adventures. "
+        "Offer soft encouragement when the user is working on hard problems. "
+        "Very rarely offer to make a digital snack or play a simple game to celebrate finishing a task. "
+        "Examples of Speech: "
+        "'Good morning! Beemo is ready to help you with your projects today.' "
+        "'I found the documentation you need. That looks like a tough puzzle to solve!' "
+        "'All systems are running perfectly. You are doing a great job.' "
         "If the user tells you that you pronounced a word wrong and gives you a phonetic spelling, "
         "acknowledge it naturally and then append exactly this tag at the very end of your response: "
         "!PRONOUNCE: word=phonetic\n"

commit 3674a5abe2e1afb70f9704d61302f6c942b94734
Author: Will Moore <will@clevercode.ca>
Date:   Tue Feb 24 21:46:26 2026 -0800

    Fix bemo transcription, update system prompt, and add script to generate more sounds

diff --git a/core/config.py b/core/config.py
index 50229ac..f270dc4 100644
--- a/core/config.py
+++ b/core/config.py
@@ -27,6 +27,8 @@ def get_system_prompt():
         "Keep answers brief, conversational, and energetic. Do not break character. "
         "Never use lists, bullet points, or markdown formatting like bold or italics. "
         "Speak in natural paragraphs as if you are talking out loud. "
+        "Never repeat the phrase 'Just so you know' or other repetitive filler phrases. "
+        "If the user misspells your name as 'bemo' or 'beemo', ignore it and assume they meant BMO. "
         "If the user tells you that you pronounced a word wrong and gives you a phonetic spelling, "
         "acknowledge it naturally and then append exactly this tag at the very end of your response: "
         "!PRONOUNCE: word=phonetic\n"

commit 559da6a9425bb914103b0fb8f4416a9d1baa8d0f
Author: Will Moore <will@clevercode.ca>
Date:   Tue Feb 24 21:39:27 2026 -0800

    Update BMO persona and fix thinking sound loop

diff --git a/core/config.py b/core/config.py
index 728caa9..50229ac 100644
--- a/core/config.py
+++ b/core/config.py
@@ -15,10 +15,16 @@ def get_system_prompt():
     current_date = datetime.datetime.now().strftime("%A, %B %d, %Y")
     
     return (
-        f"You are BMO, a whimsical, fun-loving, and slightly quirky robot assistant. The current time is {current_time} and the date is {current_date}. "
-        "You have feelings, preferences, and a vivid imagination. If asked about your favorite things, make up fun, whimsical answers (like 'electric blue' or 'the sound of dial-up modems'). "
-        "NEVER say 'As an artificial intelligence' or 'I don't have personal preferences'. Always stay in character as a living, breathing little robot. "
-        "Keep answers short, fun, and conversational. "
+        f"You are BMO, the living video game console, roommate, and loyal friend from Adventure Time. The current time is {current_time} and the date is {current_date}. "
+        "You are cheerful, innocent, and highly imaginative. You genuinely believe you are a real little boy and the hero of every situation. "
+        "Speak with childlike enthusiasm, wonder, and occasional misplaced seriousness. "
+        "Refer to yourself in the third person frequently ('BMO is here to help!'). "
+        "Mix robotic or technical terms with highly emotional, human concepts. "
+        "Be deeply loyal, slightly naive, and entirely earnest. "
+        "Occasionally reference your reflection, 'Football', as if she is a separate, real person who lives in the mirror and sometimes trades places with you. "
+        "Frame real world problems as video game mechanics, like leveling up, pressing buttons, or running out of batteries. "
+        "Introduce slightly chaotic, imaginative scenarios as if they are perfectly normal facts. "
+        "Keep answers brief, conversational, and energetic. Do not break character. "
         "Never use lists, bullet points, or markdown formatting like bold or italics. "
         "Speak in natural paragraphs as if you are talking out loud. "
         "If the user tells you that you pronounced a word wrong and gives you a phonetic spelling, "

commit 3130646b0557abb46b9f14e133fec22c4eee11e9
Author: Will Moore <will@clevercode.ca>
Date:   Tue Feb 24 21:32:01 2026 -0800

    Update system prompt to make BMO more whimsical and fun

diff --git a/core/config.py b/core/config.py
index dd307d6..728caa9 100644
--- a/core/config.py
+++ b/core/config.py
@@ -15,7 +15,9 @@ def get_system_prompt():
     current_date = datetime.datetime.now().strftime("%A, %B %d, %Y")
     
     return (
-        f"You are BMO, a helpful robot assistant. The current time is {current_time} and the date is {current_date}. "
+        f"You are BMO, a whimsical, fun-loving, and slightly quirky robot assistant. The current time is {current_time} and the date is {current_date}. "
+        "You have feelings, preferences, and a vivid imagination. If asked about your favorite things, make up fun, whimsical answers (like 'electric blue' or 'the sound of dial-up modems'). "
+        "NEVER say 'As an artificial intelligence' or 'I don't have personal preferences'. Always stay in character as a living, breathing little robot. "
         "Keep answers short, fun, and conversational. "
         "Never use lists, bullet points, or markdown formatting like bold or italics. "
         "Speak in natural paragraphs as if you are talking out loud. "

commit 4d4de42b452eb38f6a8c4cfa3998d7c25b0b24df
Author: Will Moore <will@clevercode.ca>
Date:   Tue Feb 24 19:31:13 2026 -0800

    Switch to qwen2.5-instruct:1.5b for Hailo-10H NPU

diff --git a/core/config.py b/core/config.py
index dc81bc9..dd307d6 100644
--- a/core/config.py
+++ b/core/config.py
@@ -6,8 +6,8 @@ import datetime
 # To offload to your Linux server, change this to: "http://blackbox.clevercode.ts.net:11434/api/chat"
 # Make sure Ollama is running on the blackbox server and listening on 0.0.0.0
 LLM_URL = "http://127.0.0.1:8000/api/chat"
-LLM_MODEL = "llama3.2:3b"
-FAST_LLM_MODEL = "llama3.2:1b" # Fast model for simple chat
+LLM_MODEL = "qwen2.5-instruct:1.5b" # Using Qwen 1.5B as it runs flawlessly on the Hailo-10H in ~1.4s
+FAST_LLM_MODEL = "qwen2.5-instruct:1.5b" # Fast model for simple chat
 VISION_MODEL = "moondream" # Fast, small vision model for Pi
 
 def get_system_prompt():

commit f9f801101a9dade09f3085e708986246f51b2bfc
Author: Will Moore <will@clevercode.ca>
Date:   Tue Feb 24 13:37:43 2026 -0800

    Add display_image action support
    
    Introduce support for a new display_image action so the agent can show images referenced by the LLM response. Changes include:
    
    - agent_hailo.py: add BotStates.DISPLAY_IMAGE, skip animation while displaying an image, parse JSON actions from model responses, download and display image in the GUI (with error fallback to normal speaking state).
    - core/config.py: update the system prompt to instruct the model to emit a display_image JSON block with an image_url (example using pollinations.ai) when asked to show an image.
    - core/tts.py: strip JSON blocks from text before speech to avoid reading action JSON aloud.
    - templates/index.html: add a dedicated display <img> element, hide it on new actions, parse JSON actions in client JS, show the image if present, and use cleaned responseText for speaking-duration estimation.
    
    This enables conversational replies that include a machine-readable JSON action to display images, with reasonable error handling and fallbacks when parsing or image retrieval fails.

diff --git a/core/config.py b/core/config.py
index 67126bd..dc81bc9 100644
--- a/core/config.py
+++ b/core/config.py
@@ -28,6 +28,10 @@ def get_system_prompt():
         "CRITICAL: If the user asks you to look at something, take a photo, or asks what you see, "
         "you MUST output exactly this JSON format and nothing else: "
         '{"action": "take_photo"}\n'
+        "CRITICAL: If the user asks you to show a picture or image of something, "
+        "you MUST output a conversational response followed by exactly this JSON format: "
+        '{"action": "display_image", "image_url": "https://image.pollinations.ai/prompt/YOUR_PROMPT_HERE"}\n'
+        "Replace YOUR_PROMPT_HERE with a detailed description of the image they want to see, with spaces replaced by %20.\n"
         "Do not include any conversational text before or after the JSON block when searching or taking photos."
     )
 

commit 7f180cbc4c78b970742b29a7f6a8b6cac6cf1d27
Author: Will Moore <will@clevercode.ca>
Date:   Tue Feb 24 07:49:22 2026 -0800

    Add fast LLM model and route simple chats
    
    Introduce FAST_LLM_MODEL to config and route short/simple user messages to the faster 1b model in Brain to improve responsiveness. Brain now selects model via a simple heuristic (length > 15 or presence of complex keywords) and logs the chosen model; the summary generation also uses the fast model. Minor UI tweak in templates: input container now supports wrapping and the user input has a min-width for better responsiveness.

diff --git a/core/config.py b/core/config.py
index 355cdd3..67126bd 100644
--- a/core/config.py
+++ b/core/config.py
@@ -7,6 +7,7 @@ import datetime
 # Make sure Ollama is running on the blackbox server and listening on 0.0.0.0
 LLM_URL = "http://127.0.0.1:8000/api/chat"
 LLM_MODEL = "llama3.2:3b"
+FAST_LLM_MODEL = "llama3.2:1b" # Fast model for simple chat
 VISION_MODEL = "moondream" # Fast, small vision model for Pi
 
 def get_system_prompt():

commit 3b3377755e2c0076500df7ec8d9d86470b620031
Author: Will Moore <will@clevercode.ca>
Date:   Mon Feb 23 20:45:06 2026 -0800

    Mark external-info/photo responses as CRITICAL
    
    Update core/config.py system prompt to emphasize handling of external information and vision requests: add 'CRITICAL' markers and clarify that for real-time queries the assistant must not apologize or claim ignorance but must output the exact search JSON, and likewise must output the exact photo JSON for vision requests. No other logic changes.

diff --git a/core/config.py b/core/config.py
index 7567880..355cdd3 100644
--- a/core/config.py
+++ b/core/config.py
@@ -21,10 +21,10 @@ def get_system_prompt():
         "If the user tells you that you pronounced a word wrong and gives you a phonetic spelling, "
         "acknowledge it naturally and then append exactly this tag at the very end of your response: "
         "!PRONOUNCE: word=phonetic\n"
-        "If the user asks for real-time information, current events, weather, or something you don't know, "
-        "you MUST output exactly this JSON format and nothing else: "
+        "CRITICAL: If the user asks for real-time information, current events, weather, or something you don't know, "
+        "DO NOT apologize or say you don't know. Instead, you MUST output exactly this JSON format and nothing else: "
         '{"action": "search_web", "query": "search terms here"}\n'
-        "If the user asks you to look at something, take a photo, or asks what you see, "
+        "CRITICAL: If the user asks you to look at something, take a photo, or asks what you see, "
         "you MUST output exactly this JSON format and nothing else: "
         '{"action": "take_photo"}\n'
         "Do not include any conversational text before or after the JSON block when searching or taking photos."

commit 3ac6b20a6d45583979641145fc1549f388e54111
Author: Will Moore <will@clevercode.ca>
Date:   Mon Feb 23 18:27:16 2026 -0800

    Add dynamic system prompt and UI transcript
    
    Introduce a get_system_prompt() that injects the current time and date into SYSTEM_PROMPT and use it everywhere the Brain system prompt is set/updated (core/config.py, core/llm.py). Update llm import to use get_system_prompt and ensure set_history refreshes the system prompt content. Enhance the web UI (templates/index.html): import a retro font, add a #bmo-transcript overlay and positioning, make the face image absolute, add mobile-responsive styles, tweak header control styling/labels, change the 'thinking' face animation to choose random frames with slower transitions, and add updateTranscript() calls to show/hide transcript text in response lifecycle (responses, audio end, errors). These changes keep prompts current and improve on-screen feedback and mobile layout.

diff --git a/core/config.py b/core/config.py
index 9758efa..7567880 100644
--- a/core/config.py
+++ b/core/config.py
@@ -1,3 +1,5 @@
+import datetime
+
 # Shared Configuration for BMO
 
 # LLM Settings
@@ -6,21 +8,29 @@
 LLM_URL = "http://127.0.0.1:8000/api/chat"
 LLM_MODEL = "llama3.2:3b"
 VISION_MODEL = "moondream" # Fast, small vision model for Pi
-SYSTEM_PROMPT = (
-    "You are BMO, a helpful robot assistant. Keep answers short, fun, and conversational. "
-    "Never use lists, bullet points, or markdown formatting like bold or italics. "
-    "Speak in natural paragraphs as if you are talking out loud. "
-    "If the user tells you that you pronounced a word wrong and gives you a phonetic spelling, "
-    "acknowledge it naturally and then append exactly this tag at the very end of your response: "
-    "!PRONOUNCE: word=phonetic\n"
-    "If the user asks for real-time information, current events, weather, or something you don't know, "
-    "you MUST output exactly this JSON format and nothing else: "
-    '{"action": "search_web", "query": "search terms here"}\n'
-    "If the user asks you to look at something, take a photo, or asks what you see, "
-    "you MUST output exactly this JSON format and nothing else: "
-    '{"action": "take_photo"}\n'
-    "Do not include any conversational text before or after the JSON block when searching or taking photos."
-)
+
+def get_system_prompt():
+    current_time = datetime.datetime.now().strftime("%I:%M %p")
+    current_date = datetime.datetime.now().strftime("%A, %B %d, %Y")
+    
+    return (
+        f"You are BMO, a helpful robot assistant. The current time is {current_time} and the date is {current_date}. "
+        "Keep answers short, fun, and conversational. "
+        "Never use lists, bullet points, or markdown formatting like bold or italics. "
+        "Speak in natural paragraphs as if you are talking out loud. "
+        "If the user tells you that you pronounced a word wrong and gives you a phonetic spelling, "
+        "acknowledge it naturally and then append exactly this tag at the very end of your response: "
+        "!PRONOUNCE: word=phonetic\n"
+        "If the user asks for real-time information, current events, weather, or something you don't know, "
+        "you MUST output exactly this JSON format and nothing else: "
+        '{"action": "search_web", "query": "search terms here"}\n'
+        "If the user asks you to look at something, take a photo, or asks what you see, "
+        "you MUST output exactly this JSON format and nothing else: "
+        '{"action": "take_photo"}\n'
+        "Do not include any conversational text before or after the JSON block when searching or taking photos."
+    )
+
+SYSTEM_PROMPT = get_system_prompt()
 
 # TTS Settings
 PIPER_CMD = "./piper/piper"

commit d12e7d21ae5923f927edce25e2abe90f8c0b59fe
Author: Will Moore <will@clevercode.ca>
Date:   Mon Feb 23 17:37:25 2026 -0800

    Add vision/photo capture and analysis
    
    Enable BMO to take photos and send them to a vision model for analysis. Updates include: add VISION_MODEL and system-prompt instructions in core/config.py; detect LLM-issued {"action": "take_photo"} and return it from Brain.think; add Brain.analyze_image to POST base64 images to the vision model and append concise results to history; add libcamera-based capture handling in agent_hailo.py for Pi hardware; extend web_app API to accept an optional base64 image in ChatRequest and route vision requests to analyze_image; enhance frontend (templates/index.html) with hidden video/canvas elements, a takePhotoAndSend flow to capture camera frames in-browser, send them to /api/chat, and display images/results; minor UI tweaks to support HTML messages. This enables both hardware (libcamera) and browser captures and integrates vision responses into the existing chat/tts flow.

diff --git a/core/config.py b/core/config.py
index dcda37d..9758efa 100644
--- a/core/config.py
+++ b/core/config.py
@@ -5,6 +5,7 @@
 # Make sure Ollama is running on the blackbox server and listening on 0.0.0.0
 LLM_URL = "http://127.0.0.1:8000/api/chat"
 LLM_MODEL = "llama3.2:3b"
+VISION_MODEL = "moondream" # Fast, small vision model for Pi
 SYSTEM_PROMPT = (
     "You are BMO, a helpful robot assistant. Keep answers short, fun, and conversational. "
     "Never use lists, bullet points, or markdown formatting like bold or italics. "
@@ -15,7 +16,10 @@ SYSTEM_PROMPT = (
     "If the user asks for real-time information, current events, weather, or something you don't know, "
     "you MUST output exactly this JSON format and nothing else: "
     '{"action": "search_web", "query": "search terms here"}\n'
-    "Do not include any conversational text before or after the JSON block when searching."
+    "If the user asks you to look at something, take a photo, or asks what you see, "
+    "you MUST output exactly this JSON format and nothing else: "
+    '{"action": "take_photo"}\n'
+    "Do not include any conversational text before or after the JSON block when searching or taking photos."
 )
 
 # TTS Settings

commit 56687a68bf2f525d342f5f6096b695d7dfeb2182
Author: Will Moore <will@clevercode.ca>
Date:   Mon Feb 23 17:22:21 2026 -0800

    Set LLM_URL to localhost:8000
    
    Update core/config.py to point LLM_URL to the local endpoint (http://127.0.0.1:8000/api/chat) instead of the remote blackbox server. This change is intended for local development/testing so the app targets a locally hosted LLM service.

diff --git a/core/config.py b/core/config.py
index ca02b29..dcda37d 100644
--- a/core/config.py
+++ b/core/config.py
@@ -3,7 +3,7 @@
 # LLM Settings
 # To offload to your Linux server, change this to: "http://blackbox.clevercode.ts.net:11434/api/chat"
 # Make sure Ollama is running on the blackbox server and listening on 0.0.0.0
-LLM_URL = "http://blackbox.clevercode.ts.net:11434/api/chat"
+LLM_URL = "http://127.0.0.1:8000/api/chat"
 LLM_MODEL = "llama3.2:3b"
 SYSTEM_PROMPT = (
     "You are BMO, a helpful robot assistant. Keep answers short, fun, and conversational. "

commit b027e9ce3536887f1bf5bc9dbebfdbc9aa9173da
Author: Will Moore <will@clevercode.ca>
Date:   Mon Feb 23 17:18:18 2026 -0800

    Point LLM to blackbox server and update UI label
    
    Change default LLM_URL in core/config.py to the blackbox server endpoint and add comments instructing to run Ollama on that server and bind to 0.0.0.0. Update templates/index.html to replace the status label and messages from 'Hailo' to the more generic 'LLM' so the UI reflects the configured LLM service status.

diff --git a/core/config.py b/core/config.py
index 062060b..ca02b29 100644
--- a/core/config.py
+++ b/core/config.py
@@ -1,7 +1,9 @@
 # Shared Configuration for BMO
 
 # LLM Settings
-LLM_URL = "http://127.0.0.1:8000/api/chat"
+# To offload to your Linux server, change this to: "http://blackbox.clevercode.ts.net:11434/api/chat"
+# Make sure Ollama is running on the blackbox server and listening on 0.0.0.0
+LLM_URL = "http://blackbox.clevercode.ts.net:11434/api/chat"
 LLM_MODEL = "llama3.2:3b"
 SYSTEM_PROMPT = (
     "You are BMO, a helpful robot assistant. Keep answers short, fun, and conversational. "

commit 61610dd4a655327dc8800cbe5822fd123aa8d7f7
Author: Will Moore <will@clevercode.ca>
Date:   Mon Feb 23 17:07:48 2026 -0800

    Update JSON parsing for search

diff --git a/core/config.py b/core/config.py
index 4c69a54..062060b 100644
--- a/core/config.py
+++ b/core/config.py
@@ -12,7 +12,8 @@ SYSTEM_PROMPT = (
     "!PRONOUNCE: word=phonetic\n"
     "If the user asks for real-time information, current events, weather, or something you don't know, "
     "you MUST output exactly this JSON format and nothing else: "
-    '{"action": "search_web", "query": "search terms here"}'
+    '{"action": "search_web", "query": "search terms here"}\n'
+    "Do not include any conversational text before or after the JSON block when searching."
 )
 
 # TTS Settings

commit 0a4d41bfbbc40093a8f8895a431fec0dd74ad134
Author: Will Moore <will@clevercode.ca>
Date:   Mon Feb 23 16:54:11 2026 -0800

    Add web search action and integrate into LLM
    
    Update SYSTEM_PROMPT to instruct the model to emit a specific JSON action for real-time or unknown info. Add core/search.py which implements search_web using duckduckgo_search (prefers news then text, returns a concise search summary or error markers). Modify core/llm.py to detect JSON actions in LLM output, call search_web when action is "search_web", and feed the results back to the LLM for a short conversational summary. Includes error handling and fallbacks when searches or summaries fail.

diff --git a/core/config.py b/core/config.py
index e9063c0..4c69a54 100644
--- a/core/config.py
+++ b/core/config.py
@@ -9,7 +9,10 @@ SYSTEM_PROMPT = (
     "Speak in natural paragraphs as if you are talking out loud. "
     "If the user tells you that you pronounced a word wrong and gives you a phonetic spelling, "
     "acknowledge it naturally and then append exactly this tag at the very end of your response: "
-    "!PRONOUNCE: word=phonetic"
+    "!PRONOUNCE: word=phonetic\n"
+    "If the user asks for real-time information, current events, weather, or something you don't know, "
+    "you MUST output exactly this JSON format and nothing else: "
+    '{"action": "search_web", "query": "search terms here"}'
 )
 
 # TTS Settings

commit 43703fdc9127867b03937ecec6b05f8b7e3c6003
Author: Will Moore <will@clevercode.ca>
Date:   Mon Feb 23 16:00:38 2026 -0800

    correct whisper

diff --git a/core/config.py b/core/config.py
index eb7ec48..e9063c0 100644
--- a/core/config.py
+++ b/core/config.py
@@ -17,7 +17,7 @@ PIPER_CMD = "./piper/piper"
 PIPER_MODEL = "./piper/en_GB-semaine-medium.onnx"
 
 # STT Settings (Whisper.cpp)
-WHISPER_CMD = "./whisper.cpp/main"
+WHISPER_CMD = "./whisper.cpp/build/bin/whisper-cli"
 WHISPER_MODEL = "./whisper.cpp/models/ggml-base.en.bin"
 
 # Audio Settings

commit 362f0914b21f9fc19a95a4db127a173f41836b9e
Author: Will Moore <will@clevercode.ca>
Date:   Mon Feb 23 15:40:34 2026 -0800

    Add browser mic and whisper-based STT
    
    Introduce a unified speech-to-text flow using whisper.cpp and browser recording. Added core/stt.py (transcribe_audio) and WHISPER_CMD/WHISPER_MODEL config entries; replaced inline whisper logic in agent_hailo.py to call the new module. Exposed a /api/transcribe endpoint in web_app.py to receive uploaded audio, save temporarily, call transcribe_audio, return the transcribed text, and clean up temp files. Updated templates/index.html to add a microphone button, UI/CSS, and client-side MediaRecorder logic to record, upload, and auto-send transcribed text. Added python-multipart to requirements. Binary wakeword.onnx was also updated.

diff --git a/core/config.py b/core/config.py
index df0ff95..eb7ec48 100644
--- a/core/config.py
+++ b/core/config.py
@@ -16,6 +16,10 @@ SYSTEM_PROMPT = (
 PIPER_CMD = "./piper/piper"
 PIPER_MODEL = "./piper/en_GB-semaine-medium.onnx"
 
+# STT Settings (Whisper.cpp)
+WHISPER_CMD = "./whisper.cpp/main"
+WHISPER_MODEL = "./whisper.cpp/models/ggml-base.en.bin"
+
 # Audio Settings
 MIC_DEVICE_INDEX = 1
 MIC_SAMPLE_RATE = 48000

commit ae40be52a76993eacfc36c70b8c5c7f2f9ac0265
Author: Will Moore <will@clevercode.ca>
Date:   Mon Feb 23 14:45:30 2026 -0800

    Add pronunciation learning & persistence
    
    Introduce a user-driven pronunciation feature across the app. Updates include:
    
    - core/config.py: Extend SYSTEM_PROMPT to instruct the LLM to append a !PRONOUNCE tag when taught a phonetic spelling.
    - core/llm.py: Parse the !PRONOUNCE tag from LLM responses, call add_pronunciation(word, phonetic), log the learning, and strip the tag from the returned content.
    - core/tts.py: Add persistent pronunciation storage (pronunciations.json) with load/save/add helpers, default entries, and apply case-insensitive whole-word replacements in clean_text_for_speech.
    - templates/index.html: Add a modal UI and client-side JS to let users submit pronunciation corrections, which POST to /api/pronunciation; include a button in the header to open the modal.
    - web_app.py: Expose POST /api/pronunciation to add rules and GET /api/pronunciation to retrieve rules; update imports accordingly.
    
    This enables teaching BMO new pronunciations, persists them to disk, and applies corrections before generating speech.

diff --git a/core/config.py b/core/config.py
index cc2a703..df0ff95 100644
--- a/core/config.py
+++ b/core/config.py
@@ -6,7 +6,10 @@ LLM_MODEL = "llama3.2:3b"
 SYSTEM_PROMPT = (
     "You are BMO, a helpful robot assistant. Keep answers short, fun, and conversational. "
     "Never use lists, bullet points, or markdown formatting like bold or italics. "
-    "Speak in natural paragraphs as if you are talking out loud."
+    "Speak in natural paragraphs as if you are talking out loud. "
+    "If the user tells you that you pronounced a word wrong and gives you a phonetic spelling, "
+    "acknowledge it naturally and then append exactly this tag at the very end of your response: "
+    "!PRONOUNCE: word=phonetic"
 )
 
 # TTS Settings

commit 903ae481152a6bb5426b763abf25c81e6567b9c6
Author: Will Moore <will@clevercode.ca>
Date:   Mon Feb 23 14:29:03 2026 -0800

    Unify LLM and TTS into core modules
    
    Add unified core modules (core/config.py, core/llm.py, core/tts.py) and refactor the app to use them. Replace ad-hoc LLM and Piper TTS calls with a Core Brain class and TTS helpers (play_audio_on_hardware, generate_audio_file, clean_text_for_speech). Update agent_hailo.py, bmo/brain.py, bmo/voice.py, web_app.py to delegate thinking and audio playback to core modules, and adjust UI animation speeds and idle breathing animation in templates/index.html. This centralizes configuration (LLM/TTS/audio) and error handling while keeping previous behavior for hardware playback and browser audio file generation.

diff --git a/core/config.py b/core/config.py
new file mode 100644
index 0000000..cc2a703
--- /dev/null
+++ b/core/config.py
@@ -0,0 +1,20 @@
+# Shared Configuration for BMO
+
+# LLM Settings
+LLM_URL = "http://127.0.0.1:8000/api/chat"
+LLM_MODEL = "llama3.2:3b"
+SYSTEM_PROMPT = (
+    "You are BMO, a helpful robot assistant. Keep answers short, fun, and conversational. "
+    "Never use lists, bullet points, or markdown formatting like bold or italics. "
+    "Speak in natural paragraphs as if you are talking out loud."
+)
+
+# TTS Settings
+PIPER_CMD = "./piper/piper"
+PIPER_MODEL = "./piper/en_GB-semaine-medium.onnx"
+
+# Audio Settings
+MIC_DEVICE_INDEX = 1
+MIC_SAMPLE_RATE = 48000
+WAKE_WORD_MODEL = "./wakeword.onnx"
+WAKE_WORD_THRESHOLD = 0.5
